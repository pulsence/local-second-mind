# Todo

## Testing 
- The test module structure should match the code module structure
- Test code coverage needs to be improved (target 80% test coverage)
- Run full pytest suite and verify all tests pass
- Create synthetic test data to run integration tests
- Move to a Test-Driven-Design process where for every feature non-ui that is
  created a test is first created for the feature. And then the development of
  that feature is not completed until the test is passing.

## Architecture
- General code clean up:
    - Check to make sure there is not unnecessary duplicated code
    - Check for files that are quite large and how they can be intelligently split into sub-modules
    - Remove ALL legacy and depracted code paths
    - Check the implimentations of the ai providers and see if the base
      BaseLLMProvider could encapsulate some of the common functionality
      or if it should be moved into some kind of helper.
- Refactor lsm.ingest module:
    - This module should be structure in a similiar way to lsm.query
    - It should have be entirely agnostic to who is calling
    - All output formating should be removed from the module in moved to a proper
      place in the lsm.ui module
    - lsm.ingest.display.py needs to be removed and properly split into either lsm.ui.utils
      or into lsm.ui.tui or into lsm.ui.shell
    - lsm.ingest.commands.py needs to be removed and its functionality split a part.
    - All string command parsing in lsm.ingest.commands.py needs to be handled either in lsm.ui.util or
      lsm.ui.tui or lsm.ui.shell
    - All business logic in lsm.ingest.commands.py should be moved to the proper ui modules if it is
      ui logic or refactored into helper methods in the lsm.ingest modules. These methods should take
      proper parameters (e.g., int, dict, obj, etc.) and return proper values (e.g., int, list, etc.).
      All formatting should be handled by the called of these methods.
    - lsm.ingest.config.py should be compared with the lsm.config module and either merged into that module
      or deleted it that lsm.ingest.config.py is redundant.
- Check for places to improve error handling and fault resilience
- Look for places to lazy load modules to improve start time
- Create a global default folder when one is not specified
    - The global default folder for this project when one is not specified should be
      in the local system user folder: `<USER_FOLDER>/Local Second Mind/`.
      Whenever relative folder paths for data files (e.g., notes, chats, etc.) are
      used, it should be assumed to be relative to the global folder.
    - The user should be able to set this global folder in the config.
- The configuration needs to be updated:
    - The "remote_providers" should not have an "enabled" field. A provider is enabled
      by the query mode specifying which remote_providers to use.
    - Legacies llm config fields like "model", "temperature", and "max_tokens" need to be
      removed since these fields are to be configured in the feature config of the llm provider.
    - The model classes in the lsm.config model need to be analyzed for any duplicate or redundant code.
- Congfiguration needs to be entirely reconsidered.
    - Outline the current configurations needed
    - Provide a clear class base structure to model these configurations. Start by designing the
      configuration needs in the python code first.
    - Once the python configuration models are designed, then design an intelligent json/yaml
      structure to match the python models.
    - At each step along the way consulte with the user to make sure the design is done according to
      his designs.
    - NO LEGACY OR BACKWARDS COMPATABILITY CODE SHOULD BE GENERATED! This is a clean break from the
      the past.
    - Before any code is changed to update the configuration pipeline, test code must be written to
      handle the config settings and this change is not finished until all tests are passing

## TUI
- Settings screen needs to be entirely restructured to implement the new configurations

## AI providers
- Figure out how to find out AI Provider pricing through an API call. If not possible, then create
  dict in each service provider with the current prices (e.g., scan for prices on the service provider
  pages) for current models. Then use this dict to create price estimates. The AI providers themselves
  are to do the price calculations.

## Ingest
- Add embeddings model selection (allow user to choose different embedding models)
- Add language detection and tagging for multilingual corpora
- Add option to machine translate chunks (for cross-language search)
- Consider version control for chunks (track document evolution over time)
- Enable root folders in config to be tagged to aid in identifying the kind of content in folders
- Enable tagging by folder of chunks
- Still getting MuPDF error: library error: zlib error: incorrect header check
- Improve Document ingesting
    - Store page number as chunk metadata as best able of chunks and then when a chunk is cited or
      shown, include the page number. If a chunk spans two pages it should be stored in the metadata
      as `START_#-END_#`.
    - Text chunking should be based upon internal document structure. Headings, paragraphs, and
      sentences should be treated as first class citizens. A chuck should NEVER split a sentence into
      two. A chunk should either contain a complete paragraph or a paragraph should be split into whole
      chunks, a chunk should NEVER include two different paragraphs. When headings are present the
      heading title should be stored in the Chunk metadata. A chunk should NEVER include two different
      headings.
- Enable partial ingest by time and file count
- /stats should cache results and be able to be updated incrementally

## Query
- Complete remaining documentation for query modes, notes, remote sources
- Implement query caching to be efficient in call usage
- Context building improvements:
    - User should be able to select a document (or chunk) as part of the base user prompt and then
      have the context build based upon that starting point.
    - When building the context of local embedding the metadata of the chunks should
      be used to select chunks.
    - Add AI hook to extract suggested tags from base user prompt and then use that in searching the
      local embeddings.
- Create a way to turn natural language into properly formatted queries for remote providers.
    - There should be an "intelligent" deterministic way to extract query fields from a prompt.
    - There should also be an AI provider way where the provider takes the prompt and then
      extracts the query fields from the prompt.
    - Each remote provider should track what fields their search field accepts and be able to
      properly format a query string based upon those fields. Example fields: title, author,
      keyword, etc. Then the remote provider would take those fields and turn them into a proper
      query string for that field. For example: `?q=keywords="origins, universe, singularity"&author="Hawking"`
    - Example Flow:
            - User provides Query: How does Thomas Aquinas understand this role of the Eucharist in the spiritual life?
            - Prompt to query fields: author=Thomas Aquinas, keywords: eucharist, spirituality, sacraments and spiritual life.
            - Query fields then given to remote provider who then formats the correct query string
- Query chat modes: Chat or Single-shot
- There should be a global setting for chats that sets chat saving parameters. The default save
  location should be `<GLOBAL_FOLDER/Chats/`
- Query modes can then adjust this save location, or if automatic saves happen.
- Notes settings should be set global in the config not per mode. 
- Notes should default to `<GLOBAL_FOLDER/Notes/` when none is set.
- Query modes can then set subfolder locations in the global notes folder and other parameters

## Modes and Agents
- Multi-hop reasoning for research mode
- Research into creating agents. Right now "modes" are like what other call agents,
  but really "modes" in LSM are configurations on building particular contexts. An
  agent will be a "mini-program" or "sub-routine" that can chain multiple modes
  or act upon responses to the produce further responses. And so a "mode" results
  in a single response. An agent can result in multiple response and even other
  results like the production of artifacts (e.g., files) and modifications of
  the users system itself (in theory).
- Provide ability to chain providers together in a meta-provider when the sub providers act a inputs
  to later providers and then the whole meta-provider becomes a source for query. Sources need to
  be maintained throughout the meta-provider
- Create OpenAlex - Crossref - Unpaywall - CORE pipeline to find, download, and ingest documents
- Sentiment analysis tool

## Remote Source Providers
- Look into AI Providers Web Search API as alts to Brave
- Perseus CTS API
- Digital Public Library of America (DPLA) API
- Library of Congress (loc.gov) JSON/YAML API
- Smithsonian Open Access API
- The Met Collection API (Open Access)
- Rijksmuseum data services
- IIIF APIs (Image / Presentation / Content Search
- Wikidata SPARQL endpoint
- PubMed/PubMed Central
- SSRN (Social Science Research Network)
- PhilArchive
- Project MUSE (humanities)
- News Sources: NYTimes, et al.
- Social Media: Twitter, et al.
- Finantial Sources
    - Market data
    - Federal/Government API data

## General
- Do research on particular models for particular tasks, for example using transformers directly
- Look into how to run models in app
- Explore multimodal capabilities (images, audio via Whisper)