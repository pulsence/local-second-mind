# Todo

## Architecture

## TUI
- Settings screen needs to be entirely restructured to implement the new configurations

## AI providers
- Figure out how to find out AI Provider pricing through an API call. If not possible, then create
  dict in each service provider with the current prices (e.g., scan for prices on the service provider
  pages) for current models. Then use this dict to create price estimates. The AI providers themselves
  are to do the price calculations.

## Ingest
- Add embeddings model selection (allow user to choose different embedding models)
- Add language detection and tagging for multilingual corpora
- Add option to machine translate chunks (for cross-language search)
- Consider version control for chunks (track document evolution over time)
- Enable root folders in config to be tagged to aid in identifying the kind of content in folders
- Enable tagging by folder of chunks
- Still getting MuPDF error: library error: zlib error: incorrect header check
- Improve Document ingesting
    - Store page number as chunk metadata as best able of chunks and then when a chunk is cited or
      shown, include the page number. If a chunk spans two pages it should be stored in the metadata
      as `START_#-END_#`.
    - Text chunking should be based upon internal document structure. Headings, paragraphs, and
      sentences should be treated as first class citizens. A chuck should NEVER split a sentence into
      two. A chunk should either contain a complete paragraph or a paragraph should be split into whole
      chunks, a chunk should NEVER include two different paragraphs. When headings are present the
      heading title should be stored in the Chunk metadata. A chunk should NEVER include two different
      headings.
    - Once the base chunk amount is settled by the above criteria then there should be a 20-30% overlap.
- Enable partial ingest by time and file count
- /stats should cache results and be able to be updated incrementally

## Query
- Complete remaining documentation for query modes, notes, remote sources
- Implement query caching to be efficient in call usage
- Context building improvements:
    - User should be able to select a document (or chunk) as part of the base user prompt and then
      have the context build based upon that starting point.
    - When building the context of local embedding the metadata of the chunks should
      be used to select chunks.
    - Add AI hook to extract suggested tags from base user prompt and then use that in searching the
      local embeddings.
- Create a way to turn natural language into properly formatted queries for remote providers.
    - There should be an "intelligent" deterministic way to extract query fields from a prompt.
    - There should also be an AI provider way where the provider takes the prompt and then
      extracts the query fields from the prompt.
    - Each remote provider should track what fields their search field accepts and be able to
      properly format a query string based upon those fields. Example fields: title, author,
      keyword, etc. Then the remote provider would take those fields and turn them into a proper
      query string for that field. For example: `?q=keywords="origins, universe, singularity"&author="Hawking"`
    - Example Flow:
            - User provides Query: How does Thomas Aquinas understand this role of the Eucharist in the spiritual life?
            - Prompt to query fields: author=Thomas Aquinas, keywords: eucharist, spirituality, sacraments and spiritual life.
            - Query fields then given to remote provider who then formats the correct query string
- Query chat modes: Chat or Single-shot
- There should be a global setting for chats that sets chat saving parameters. The default save
  location should be `<GLOBAL_FOLDER/Chats/`
- Query modes can adjust if automatic saves happen.
- Query modes can then set subfolder locations in the global notes folder and other parameters

## Modes and Agents
- Multi-hop reasoning for research mode
- Research into creating agents. Right now "modes" are like what other call agents,
  but really "modes" in LSM are configurations on building particular contexts. An
  agent will be a "mini-program" or "sub-routine" that can chain multiple modes
  or act upon responses to the produce further responses. And so a "mode" results
  in a single response. An agent can result in multiple response and even other
  results like the production of artifacts (e.g., files) and modifications of
  the users system itself (in theory).
- Provide ability to chain providers together in a meta-provider when the sub providers act a inputs
  to later providers and then the whole meta-provider becomes a source for query. Sources need to
  be maintained throughout the meta-provider
- Create OpenAlex - Crossref - Unpaywall - CORE pipeline to find, download, and ingest documents
- Sentiment analysis tool

## Remote Source Providers
- Look into AI Providers Web Search API as alts to Brave
- Perseus CTS API
- Digital Public Library of America (DPLA) API
- Library of Congress (loc.gov) JSON/YAML API
- Smithsonian Open Access API
- The Met Collection API (Open Access)
- Rijksmuseum data services
- IIIF APIs (Image / Presentation / Content Search
- Wikidata SPARQL endpoint
- PubMed/PubMed Central
- SSRN (Social Science Research Network)
- PhilArchive
- Project MUSE (humanities)
- News Sources: NYTimes, et al.
- Social Media: Twitter, et al.
- Finantial Sources
    - Market data
    - Federal/Government API data

## General
- Do research on particular models for particular tasks, for example using transformers directly
- Look into how to run models in app
- Explore multimodal capabilities (images, audio via Whisper)